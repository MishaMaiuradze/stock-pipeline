version: '3.8'

services:
  # Kafka broker (KRaft mode - no Zookeeper)
  kafka:
    image: apache/kafka:3.7.0
    container_name: kafka
    ports:
      - "9092:9092"
      - "9093:9093"
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_NUM_PARTITIONS: 3
      CLUSTER_ID: 'MkU3OEVBNTcwNTJENDM2Qk'
    volumes:
      - kafka-data:/var/lib/kafka/data
    networks:
      - stock-network
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost 9092 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s

  # PostgreSQL Database
  postgres:
    image: postgres:16
    container_name: postgres
    environment:
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: admin123
      POSTGRES_DB: stock_data
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./init-scripts:/docker-entrypoint-initdb.d
    networks:
      - stock-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U admin -d stock_data"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Apache Flink JobManager
  flink-jobmanager:
    image: flink:1.18.1-scala_2.12-java11
    container_name: flink-jobmanager
    ports:
      - "8081:8081"
    command: jobmanager
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: flink-jobmanager
        state.backend: filesystem
        state.checkpoints.dir: file:///tmp/flink-checkpoints
        state.savepoints.dir: file:///tmp/flink-savepoints
        execution.checkpointing.interval: 60000
    volumes:
      - flink-checkpoints:/tmp/flink-checkpoints
      - flink-savepoints:/tmp/flink-savepoints
      - ./flink-jobs:/opt/flink/usrlib
    networks:
      - stock-network

  # Apache Flink TaskManager
  flink-taskmanager:
    image: flink:1.18.1-scala_2.12-java11
    container_name: flink-taskmanager
    depends_on:
      - flink-jobmanager
    command: taskmanager
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: flink-jobmanager
        taskmanager.numberOfTaskSlots: 2
        state.backend: filesystem
        state.checkpoints.dir: file:///tmp/flink-checkpoints
        state.savepoints.dir: file:///tmp/flink-savepoints
    volumes:
      - flink-checkpoints:/tmp/flink-checkpoints
      - flink-savepoints:/tmp/flink-savepoints
      - ./flink-jobs:/opt/flink/usrlib
    networks:
      - stock-network

  # Apache Spark Master
  spark-master:
    build:
      context: ./spark-master
      dockerfile: Dockerfile
    container_name: spark-master
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - spark-apps:/opt/spark-apps
      - spark-data:/opt/spark-data
    networks:
      - stock-network
    environment:
      - SPARK_NO_DAEMONIZE=true

  # Apache Spark Worker
  spark-worker:
    build:
      context: ./spark-master
      dockerfile: Dockerfile
    container_name: spark-worker
    depends_on:
      - spark-master
    volumes:
      - spark-apps:/opt/spark-apps
      - spark-data:/opt/spark-data
    networks:
      - stock-network
    environment:
      - SPARK_NO_DAEMONIZE=true
      - SPARK_MASTER=spark://spark-master:7077
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077

  # Binance Data Producer
  binance-producer:
    build:
      context: ./producer
      dockerfile: Dockerfile
    container_name: binance-producer
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      KAFKA_TOPIC: binance-prices
      BINANCE_SYMBOLS: BTCUSDT,ETHUSDT,BNBUSDT,ADAUSDT,SOLUSDT
    networks:
      - stock-network
    restart: unless-stopped

  # Flink Stream Processor
  flink-processor:
    build:
      context: ./flink-processor
      dockerfile: Dockerfile
    container_name: flink-processor
    depends_on:
      - kafka
      - postgres
      - flink-jobmanager
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      KAFKA_TOPIC: binance-prices
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_DB: stock_data
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: admin123
    networks:
      - stock-network
    restart: unless-stopped

  # Spark Batch Processor
  spark-processor:
    build:
      context: ./spark-processor
      dockerfile: Dockerfile
    container_name: spark-processor
    depends_on:
      - spark-master
      - postgres
    environment:
      SPARK_MASTER_URL: spark://spark-master:7077
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_DB: stock_data
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: admin123
    volumes:
      - spark-apps:/opt/spark-apps
    networks:
      - stock-network
    restart: unless-stopped

  # Apache Superset
  superset:
    image: apache/superset:3.1.0
    container_name: superset
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      SUPERSET_SECRET_KEY: 'your-secret-key-change-in-production'
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_DB: stock_data
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: admin123
    ports:
      - "8088:8088"
    volumes:
      - superset-data:/app/superset_home
      - ./superset-init:/app/docker-init
    networks:
      - stock-network
    command: >
      bash -c "
      pip install psycopg2-binary &&
      superset db upgrade &&
      superset fab create-admin --username admin --firstname Admin --lastname User --email admin@superset.com --password admin123 &&
      superset init &&
      /usr/bin/run-server.sh
      "
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8088/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

networks:
  stock-network:
    driver: bridge

volumes:
  kafka-data:
  postgres-data:
  flink-checkpoints:
  flink-savepoints:
  spark-apps:
  spark-data:
  superset-data:
